---
title: "Databases"
author: ["Kirill & Nicolas", "cynkra GmbH"]
date: "March 15, 2022"
output:
  cynkradown::cynkra_slides:
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
    seal: true
lang: english
font: frutiger
wide: false
colorlinks: false
logo: true
---

<style type="text/css">
.pull-left {
  margin-top: -25px;
}
.pull-right {
  margin-top: -25px;
}
.remark-code {
    font-size: 17px;
}
.font17 {
    font-size: 17px;
}
.font14 {
    font-size: 14px;
}
</style>


```{r databases-1, include = FALSE}
knitr::opts_chunk$set(cache = TRUE)

library(tidyverse)
library(DBI)
library(dm)

options(width = 160)

options(pillar.bold = TRUE)
options(cli.num_colors = 256)

fansi::set_knit_hooks(knitr::knit_hooks, c("output", "message", "warning", "error"))
```


# Introduction

Organization of half-day R courses:

- Intro courses:
  * Tidyverse intro I
  * Base R intro/Tidyverse intro II
  * Data visualization I
  * Data visualization II
- Advanced courses:
  * Advanced tidyverse
  * R package creation
  * Working with database systems
  * Parallelization & efficient R programming
  * Databases (this course)

---

# Course material

Our course material currently is available from GitHub at

https://github.com/cynkra/bag-courses

Today we will be looking at the folder `5_databases`

---

# Download course material

![how to download](cynkra-repo-dl.png)

---

# General remarks

- We hope for these courses to be interactive: go ahead and ask if something is unclear!
- You can also write into the chat, which I will try to monitor when Kirill is presenting.
- We were asked to provide recordings of the courses for those of you who cannot join, so recording is activated.
- Per course unit, we offer 4 hours of follow up time; approach us with questions (nicolas@cynkra.com)!

---

# Goals for today

Playing the whole game!

- Extract
- Transform
- Load
- **Consume**

---

# Schedule

Extract ⇨ Transform ⇨ Load ⇨ Consume

.pull-left[
## One table

- Read whole tables
- Let the database do the heavy lifting
- Subtle issues to watch out for
- Basic ETL for one table
]

.pull-right[
## Multiple tables

- Joins
- The {dm} package
- A bit of theory
- Playing the whole game
]

## Bonus: Bring your own data + questions

---

# Schedule

Extract ⇨ Transform ⇨ Load ⇨ Consume

.pull-left[
## One table

- **Read whole tables**
- Let the database do the heavy lifting
- Subtle issues to watch out for
- Basic ETL for one table
]

.pull-right[
## Multiple tables

- Joins
- The {dm} package
- A bit of theory
- Playing the whole game
]

## Bonus: Bring your own data + questions

---

background-image: url("images/11.webp")
background-size: contain
background-position: 100% 100%

# Read whole tables

.pull-left[
- {DBI} package
- Connect
- Discover
- Read
- Query

Script: `databases_11.R`

```{r databases-2 }
library(tidyverse)
library(DBI)
```
]

---

background-image: url("images/11-frame.webp")
background-size: contain
background-position: 100% 100%

# Read whole tables

.pull-left[
- {DBI} package
- Connect
- Discover
- Read
- Query

Script: `databases_11.R`

```{r databases-3 }
library(tidyverse)
library(DBI)
```
]

---

# Connect to the database

First step when accessing the database.

```{r databases-4, results='hide', message=FALSE, cache=FALSE}
con_duckdb <- dbConnect(duckdb::duckdb())
con_duckdb
```

```
## <duckdb_connection 0faa0 driver=<duckdb_driver 22170 dbdir=':memory:' read_only=FALSE>>
```

---

background-image: url("images/magic-hat-and-wand-nath-r.svg")
background-size: 25%
background-position: 100% 100%

# Prepare database

Normally done in a preparatory step.

```{r databases-5, cache=FALSE}
dm::copy_dm_to(
  con_duckdb,
  dm::dm_pixarfilms(),
  set_key_constraints = FALSE,
  temporary = FALSE
)
```

---

# Discover tables

Where is my data?

```{r databases-6 }
dbListTables(con_duckdb)
dbListFields(con_duckdb, "pixar_films")
```

Caveat: schemas, catalogs, ...

```sql
SELECT * FROM INFORMATION_SCHEMA.TABLES
```

---

# Read tables

Read entire tables into your local session, if you can afford it.

```{r databases-7 }
df_pixar_films <- dbReadTable(con_duckdb, "pixar_films")
df_pixar_films
```

---

# Read tables

Use `as_tibble()` to convert to a tibble for better display
and more robust operation.

```{r databases-8 }
df_pixar_films <- dbReadTable(con_duckdb, "pixar_films")
as_tibble(df_pixar_films)
```

---

# Execute queries

Write SQL code to define what data you want to see.

```{r databases-9 }
dbGetQuery(con_duckdb, "SELECT * FROM pixar_films")
```

---

# Execute queries

Write complex SQL code to define what data you want to see.

```{r databases-10 }
sql <- "
SELECT * FROM pixar_films
WHERE release_date >= '2020-01-01'
"
```

```{r databases-11 }
dbGetQuery(con_duckdb, sql)
```

---

# Execute queries

R 4.0 or later: use new-style string literals for mixing quotes.

```{r databases-12, eval = FALSE}
sql <- r"(
SELECT * FROM "pixar_films"
WHERE "release_date" >= '2020-01-01'
)"
```

```{r databases-13 }
dbGetQuery(con_duckdb, sql)
```

---

background-image: url("images/magic-hat-and-wand-nath-r.svg")
background-size: 25%
background-position: 100% 100%

# Further pointers

.pull-left[
## Quoting

```{r databases-14 }
dbQuoteIdentifier(con_duckdb, "academy")
dbQuoteLiteral(con_duckdb, "Toy Story")
dbQuoteLiteral(con_duckdb, as.Date("2020-01-01"))
```

```{r databases-15, eval = FALSE}
glue::glue_sql(...)
```
]

.pull-right[
## Parameterized queries

```{r databases-16 }
sql <- "
SELECT count(*) FROM pixar_films
WHERE release_date >= ?
"
dbGetQuery(con_duckdb, sql,
  params = list(as.Date("2020-01-01"))
)
```
]

---

background-image: url("images/11-frame.webp")
background-size: contain
background-position: 100% 100%

# Read whole tables: Exercises

.pull-left[
1. List all columns from the `box_office` table.
2. Read the `academy` table.
3. Read all records from the `academy` table that correspond to awards won
    - Hint: Use the query `"SELECT * FROM academy WHERE status = 'Won'"`
4. Use quoting and/or query parameters to stabilize the previous query.
]

---

# Schedule

Extract ⇨ Transform ⇨ Load ⇨ Consume

.pull-left[
## One table

- Read whole tables
- **Let the database do the heavy lifting**
- Subtle issues to watch out for
- Basic ETL for one table
]

.pull-right[
## Multiple tables

- Joins
- The {dm} package
- A bit of theory
- Playing the whole game
]

## Bonus: Bring your own data + questions

---

background-image: url("images/12.webp")
background-size: 40%
background-position: 100% 100%

# Downsizing on the database

.pull-left[
- {dbplyr} package (part of the tidyverse)
- Lazy tables
- `collect()`
- `select()`
- `filter()`
- `group_by()`, `summarize()`, `ungroup()`, `count()`

Script: `databases_12_1.R`

```{r databases-17 }
library(tidyverse)
```
]

---

background-image: url("images/12-frame.webp")
background-size: 40%
background-position: 100% 100%

# Downsizing on the database

.pull-left[
- {dbplyr} package (part of the tidyverse)
- Lazy tables
- `collect()`
- `select()`
- `filter()`
- `group_by()`, `summarize()`, `ungroup()`, `count()`

Script: `databases_12_1.R`

```{r databases-18 }
library(tidyverse)
```
]

---

# Lazy tables

A pointer to a SQL table. The data is still on the database!

```{r databases-19, cache = FALSE}
pixar_films <- tbl(con_duckdb, "pixar_films")
pixar_films
```

---

# Read the whole table

```{r databases-20 }
df_pixar_films <-
  pixar_films %>%
  collect()
df_pixar_films
```

---

# Select columns

With `select()`, like with data frames.

.pull-left[
```{r databases-21 }
pixar_films %>%
  select(1:3)
```
]

.pull-right[
## Under the hood

```{r databases-22 }
pixar_films %>%
  select(1:3) %>%
  show_query()
```
]

---

# Select columns and read

.pull-left[
```{r databases-23 }
df_pixar_films_3 <-
  pixar_films %>%
  select(1:3) %>%
  collect()
df_pixar_films_3
```
]

.pull-right[
## Data on the database not affected

```{r databases-24 }
pixar_films %>%
  collect()
```
]

---

# Select rows

With `filter()`, like with data frames.

.pull-left[
```{r databases-25 }
pixar_films %>%
  filter(release_date >= "2020-01-01")
```
]

.pull-right[
## Under the hood

```{r databases-26 }
pixar_films %>%
  filter(release_date >= "2020-01-01") %>%
  show_query()
```
]

---

# Select rows and read

.pull-left[
```{r databases-27 }
df_pixar_films_202x <-
  pixar_films %>%
  filter(release_date >= "2020-01-01") %>%
  collect()
df_pixar_films_202x
```
]

.pull-right[
## Data on the database not affected

```{r databases-28 }
pixar_films %>%
  collect()
```
]

---

# Aggregate

With `group_by()` + `summarize()` + `ungroup()`, like with data frames.

.pull-left[
```{r databases-29 }
pixar_films %>%
  group_by(film_rating) %>%
  summarize(n = n()) %>%
  ungroup()
```
]

.pull-right[
## Under the hood

```{r databases-30 }
pixar_films %>%
  group_by(film_rating) %>%
  summarize(n = n()) %>%
  ungroup() %>%
  show_query()
```
]

---

# Aggregate

With `count()`, like with data frames.

.pull-left[
```{r databases-31 }
pixar_films %>%
  #
  #
  count(film_rating)
```
]

.pull-right[
## Under the hood

```{r databases-32 }
pixar_films %>%
  #
  #
  count(film_rating) %>%
  show_query()
```
]

---

# Aggregate and read

.pull-left[
```{r databases-33 }
df_pixar_films_by_rating <-
  pixar_films %>%
  count(film_rating) %>%
  collect()
df_pixar_films_by_rating
```
]

.pull-right[
## Data on the database not affected

```{r databases-34 }
pixar_films %>%
  collect()
```
]

---

background-image: url("images/12-frame.webp")
background-size: 40%
background-position: 100% 100%

# Downsizing on the database: Exercises 1

*  Find several ways to select the 3 first columns
*  What happens if you include the name of a variable multiple times in a `select()` call?
*  Select all columns that contain underscores (use `contains()`)
*  Use `all_of()` to select 2 columns of your choice

---

background-image: url("images/12-frame.webp")
background-size: 40%
background-position: 100% 100%

# Downsizing on the database: Exercises 2

.pull-left[
Find all films that
1. Are rated "PG"
2. Had a run time below 95
3. Had a rating of "N/A" or "Not Rated"
4. Were released after and including year 2020
5. Have a missing name (`film` column) or `run_time`
6. Are a first sequel (the name ends with "2")
    - Hint: Bring the data into the R session before filtering
]

---

background-image: url("images/12-frame.webp")
background-size: 40%
background-position: 100% 100%

# Downsizing on the database: Exercises 3

1. How many films are stored in the table?
2. How many films released after 2005 are stored in the table?
3. What is the total run time of all films?
    - Hint: Use `summarize(sum(...))`, watch out for the warning
4. What is the total run time of all films, per rating?
    - Hint: Use `group_by()`

---

.pull-left[
]

.pull-right[
]
